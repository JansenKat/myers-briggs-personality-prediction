{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (MBTI) Myers-Briggs Personality Type Prediction\n",
    "\n",
    "* Extroversion vs. Introversion\n",
    "    * I - 0\n",
    "    * E - 1 \n",
    "    \n",
    "* Sensing vs. Intuition \n",
    "    * N - 0 \n",
    "    * S - 1\n",
    "    \n",
    "* Thinking vs. Feeling\n",
    "    * F - 0\n",
    "    * T - 1\n",
    "    \n",
    "* Judging vs. Perceiving\n",
    "    * P - 0\n",
    "    * J - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# importing dependencies here\\nimport numpy as np\\nimport pandas as pd\\n\\n# visualizations\\n# import seaborn as sns\\n# import matplotlib.pyplot as plt\\n\\n# feature engineering\\nimport re\\nimport nltk\\nfrom nltk.corpus import stopwords\\n\\nnltk.download(\\\"stopwords\\\")\\nnltk.download(\\\"punkt\\\")\\nnltk.download(\\\"averaged_perceptron_tagger\\\")\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\nfrom nltk.stem import WordNetLemmatizer\\n\\nnltk.download(\\\"wordnet\\\")\\nnltk.download(\\\"vader_lexicon\\\")\\n\\n# sentiment scoring\\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n\\n# scikit\\n# vectorization\\n# from sklearn.feature_extraction.text import CountVectorizer\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# # scaling to handle negative values (for Naive Bayes)\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# # data stratifying and splitting\\n# from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\\n# from sklearn.model_selection import train_test_split\\n\\n# # algorithms/models\\n# # from sklearn.pipeline import make_pipeline\\n# from sklearn.naive_bayes import GaussianNB\\n# from sklearn.naive_bayes import MultinomialNB\\n# from sklearn.linear_model import LogisticRegression\\n# from sklearn.ensemble import RandomForestClassifier\\n\\n# # model performance evaluation and selection\\n# from sklearn.metrics import (\\n#     classification_report,\\n#     f1_score,\\n#     accuracy_score,\\n#     roc_auc_score,\\n# )\\n\\n# performance check\\nimport time\\n\\n# import warnings\\n\\n# warnings.filterwarnings(\\\"ignore\\\")\\n\\n# # sparse to dense\\n# from sklearn.base import TransformerMixin\\n\\n\\n# class DenseTransformer(TransformerMixin):\\n#     def fit(self, X, y=None, **fit_params):\\n#         return self\\n\\n#     def transform(self, X, y=None, **fit_params):\\n#         return X.todense()\\n\\n\\n# code formatter\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# importing dependencies here\\nimport numpy as np\\nimport pandas as pd\\n\\n# visualizations\\n# import seaborn as sns\\n# import matplotlib.pyplot as plt\\n\\n# feature engineering\\nimport re\\nimport nltk\\nfrom nltk.corpus import stopwords\\n\\nnltk.download(\\\"stopwords\\\")\\nnltk.download(\\\"punkt\\\")\\nnltk.download(\\\"averaged_perceptron_tagger\\\")\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\nfrom nltk.stem import WordNetLemmatizer\\n\\nnltk.download(\\\"wordnet\\\")\\nnltk.download(\\\"vader_lexicon\\\")\\n\\n# sentiment scoring\\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n\\n# scikit\\n# vectorization\\n# from sklearn.feature_extraction.text import CountVectorizer\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# # scaling to handle negative values (for Naive Bayes)\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# # data stratifying and splitting\\n# from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\\n# from sklearn.model_selection import train_test_split\\n\\n# # algorithms/models\\n# # from sklearn.pipeline import make_pipeline\\n# from sklearn.naive_bayes import GaussianNB\\n# from sklearn.naive_bayes import MultinomialNB\\n# from sklearn.linear_model import LogisticRegression\\n# from sklearn.ensemble import RandomForestClassifier\\n\\n# # model performance evaluation and selection\\n# from sklearn.metrics import (\\n#     classification_report,\\n#     f1_score,\\n#     accuracy_score,\\n#     roc_auc_score,\\n# )\\n\\n# performance check\\nimport time\\n\\n# import warnings\\n\\n# warnings.filterwarnings(\\\"ignore\\\")\\n\\n# # sparse to dense\\n# from sklearn.base import TransformerMixin\\n\\n\\n# class DenseTransformer(TransformerMixin):\\n#     def fit(self, X, y=None, **fit_params):\\n#         return self\\n\\n#     def transform(self, X, y=None, **fit_params):\\n#         return X.todense()\\n\\n\\n# code formatter\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing dependencies here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# feature engineering\n",
    "import re\n",
    "import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# sentiment scoring\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# # scaling to handle negative values in sentiment scores (for Naive Bayes)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# performance check\n",
    "import time\n",
    "\n",
    "# code formatter\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# reading the clean_dataset_1\\npersonality_data = pd.read_csv(\\\"data_ekta/clean_data_1.csv\\\")\";\n",
       "                var nbb_formatted_code = \"# reading the clean_dataset_1\\npersonality_data = pd.read_csv(\\\"data_ekta/clean_data_1.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reading the clean_dataset_1\n",
    "personality_data = pd.read_csv(\"data_ekta/clean_data_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>is_Extrovert</th>\n",
       "      <th>is_Sensing</th>\n",
       "      <th>is_Thinking</th>\n",
       "      <th>is_Judging</th>\n",
       "      <th>posts</th>\n",
       "      <th>clean_posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>youtube    moment  youtube  sportscenter top t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>im finding lack post alarming sex boring posit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>good one     youtube course say know thats ble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>dear    enjoyed conversation day  esoteric gab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>youre fired thats another silly misconception ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  is_Extrovert  is_Sensing  is_Thinking  is_Judging  \\\n",
       "0  INFJ             0           0            0           1   \n",
       "1  ENTP             1           0            1           0   \n",
       "2  INTP             0           0            1           0   \n",
       "3  INTJ             0           0            1           1   \n",
       "4  ENTJ             1           0            1           1   \n",
       "\n",
       "                                               posts  \\\n",
       "0  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  'I'm finding the lack of me in these posts ver...   \n",
       "2  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                         clean_posts  \n",
       "0  youtube    moment  youtube  sportscenter top t...  \n",
       "1  im finding lack post alarming sex boring posit...  \n",
       "2  good one     youtube course say know thats ble...  \n",
       "3  dear    enjoyed conversation day  esoteric gab...  \n",
       "4  youre fired thats another silly misconception ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# lookign at the top 5 rows of the dataset\\npersonality_data.head()\";\n",
       "                var nbb_formatted_code = \"# lookign at the top 5 rows of the dataset\\npersonality_data.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lookign at the top 5 rows of the dataset\n",
    "personality_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# checking the number of rows and columns\\npersonality_data.shape\";\n",
       "                var nbb_formatted_code = \"# checking the number of rows and columns\\npersonality_data.shape\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking the number of rows and columns\n",
    "personality_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type            0\n",
       "is_Extrovert    0\n",
       "is_Sensing      0\n",
       "is_Thinking     0\n",
       "is_Judging      0\n",
       "posts           0\n",
       "clean_posts     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# checking for missing values\\npersonality_data.isnull().sum()\";\n",
       "                var nbb_formatted_code = \"# checking for missing values\\npersonality_data.isnull().sum()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking for missing values\n",
    "personality_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily there are no missing values present in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiments Analysis Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CAUTION - Sentiment scoring will take LONG !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Scoring Time: 821.68 seconds\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# sentiment scoring for each user\\nt = time.time()\\n\\nanalyzer = SentimentIntensityAnalyzer()\\n\\nnlp_sentiment_score = []\\n\\nfor post in personality_data[\\\"clean_posts\\\"]:\\n    score = analyzer.polarity_scores(post)\\n    nlp_sentiment_score.append(score)\\n\\nprint(f\\\"Sentiment Scoring Time: {time.time() - t:.2f} seconds\\\")\";\n",
       "                var nbb_formatted_code = \"# sentiment scoring for each user\\nt = time.time()\\n\\nanalyzer = SentimentIntensityAnalyzer()\\n\\nnlp_sentiment_score = []\\n\\nfor post in personality_data[\\\"clean_posts\\\"]:\\n    score = analyzer.polarity_scores(post)\\n    nlp_sentiment_score.append(score)\\n\\nprint(f\\\"Sentiment Scoring Time: {time.time() - t:.2f} seconds\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sentiment scoring for each user\n",
    "t = time.time()\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "nlp_sentiment_score = []\n",
    "\n",
    "for post in personality_data[\"clean_posts\"]:\n",
    "    score = analyzer.polarity_scores(post)\n",
    "    nlp_sentiment_score.append(score)\n",
    "\n",
    "print(f\"Sentiment Scoring Time: {time.time() - t:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# segregating the indiviual sentiment scores - compound, positive, negative and neutral\\npersonality_data[\\\"compound_sentiment\\\"] = [\\n    score[\\\"compound\\\"] for score in nlp_sentiment_score\\n]\\npersonality_data[\\\"pos_sentiment\\\"] = [score[\\\"pos\\\"] for score in nlp_sentiment_score]\\npersonality_data[\\\"neg_sentiment\\\"] = [score[\\\"neg\\\"] for score in nlp_sentiment_score]\\npersonality_data[\\\"neu_sentiment\\\"] = [score[\\\"neu\\\"] for score in nlp_sentiment_score]\";\n",
       "                var nbb_formatted_code = \"# segregating the indiviual sentiment scores - compound, positive, negative and neutral\\npersonality_data[\\\"compound_sentiment\\\"] = [\\n    score[\\\"compound\\\"] for score in nlp_sentiment_score\\n]\\npersonality_data[\\\"pos_sentiment\\\"] = [score[\\\"pos\\\"] for score in nlp_sentiment_score]\\npersonality_data[\\\"neg_sentiment\\\"] = [score[\\\"neg\\\"] for score in nlp_sentiment_score]\\npersonality_data[\\\"neu_sentiment\\\"] = [score[\\\"neu\\\"] for score in nlp_sentiment_score]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# segregating the indiviual sentiment scores - compound, positive, negative and neutral\n",
    "personality_data[\"compound_sentiment\"] = [\n",
    "    score[\"compound\"] for score in nlp_sentiment_score\n",
    "]\n",
    "personality_data[\"pos_sentiment\"] = [score[\"pos\"] for score in nlp_sentiment_score]\n",
    "personality_data[\"neg_sentiment\"] = [score[\"neg\"] for score in nlp_sentiment_score]\n",
    "personality_data[\"neu_sentiment\"] = [score[\"neu\"] for score in nlp_sentiment_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# Sentiment scores have negative values that Naive Bayes can't handle. So scaling it.\\n\\nmin_max_scaler = MinMaxScaler()\\npersonality_data[\\\"compound_sentiment\\\"] = min_max_scaler.fit_transform(\\n    np.array(personality_data[\\\"compound_sentiment\\\"]).reshape(-1, 1)\\n)\\npersonality_data[\\\"pos_sentiment\\\"] = min_max_scaler.fit_transform(\\n    np.array(personality_data[\\\"pos_sentiment\\\"]).reshape(-1, 1)\\n)\\npersonality_data[\\\"neg_sentiment\\\"] = min_max_scaler.fit_transform(\\n    np.array(personality_data[\\\"neg_sentiment\\\"]).reshape(-1, 1)\\n)\\npersonality_data[\\\"neu_sentiment\\\"] = min_max_scaler.fit_transform(\\n    np.array(personality_data[\\\"neu_sentiment\\\"]).reshape(-1, 1)\\n)\";\n",
       "                var nbb_formatted_code = \"# Sentiment scores have negative values that Naive Bayes can't handle. So scaling it.\\n\\nmin_max_scaler = MinMaxScaler()\\npersonality_data[\\\"compound_sentiment\\\"] = min_max_scaler.fit_transform(\\n    np.array(personality_data[\\\"compound_sentiment\\\"]).reshape(-1, 1)\\n)\\npersonality_data[\\\"pos_sentiment\\\"] = min_max_scaler.fit_transform(\\n    np.array(personality_data[\\\"pos_sentiment\\\"]).reshape(-1, 1)\\n)\\npersonality_data[\\\"neg_sentiment\\\"] = min_max_scaler.fit_transform(\\n    np.array(personality_data[\\\"neg_sentiment\\\"]).reshape(-1, 1)\\n)\\npersonality_data[\\\"neu_sentiment\\\"] = min_max_scaler.fit_transform(\\n    np.array(personality_data[\\\"neu_sentiment\\\"]).reshape(-1, 1)\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sentiment scores have negative values that Naive Bayes can't handle. So scaling it.\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "personality_data[\"compound_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"compound_sentiment\"]).reshape(-1, 1)\n",
    ")\n",
    "personality_data[\"pos_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"pos_sentiment\"]).reshape(-1, 1)\n",
    ")\n",
    "personality_data[\"neg_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"neg_sentiment\"]).reshape(-1, 1)\n",
    ")\n",
    "personality_data[\"neu_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"neu_sentiment\"]).reshape(-1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type                  0\n",
       "is_Extrovert          0\n",
       "is_Sensing            0\n",
       "is_Thinking           0\n",
       "is_Judging            0\n",
       "posts                 0\n",
       "clean_posts           0\n",
       "compound_sentiment    0\n",
       "pos_sentiment         0\n",
       "neg_sentiment         0\n",
       "neu_sentiment         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"# checking to see if sentiment scores introduced any null value\\npersonality_data.isnull().sum()\";\n",
       "                var nbb_formatted_code = \"# checking to see if sentiment scores introduced any null value\\npersonality_data.isnull().sum()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking to see if sentiment scores introduced any null value\n",
    "personality_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"# creating tag_posts column that will have each post as a separate list in a row. tag_posts will be a list of 50 lists.  \\n\\n# replacing urls with domain name\\npersonality_data[\\\"tag_posts\\\"] = personality_data[\\\"posts\\\"].str.replace(\\n    re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n    lambda match: match.group(2),\\n)\\n\\n# replacing ||| with space\\npersonality_data[\\\"tag_posts\\\"] = [\\n    post for post in personality_data[\\\"tag_posts\\\"].str.split(\\\"\\\\|\\\\|\\\\|\\\")\\n]\";\n",
       "                var nbb_formatted_code = \"# creating tag_posts column that will have each post as a separate list in a row. tag_posts will be a list of 50 lists.\\n\\n# replacing urls with domain name\\npersonality_data[\\\"tag_posts\\\"] = personality_data[\\\"posts\\\"].str.replace(\\n    re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n    lambda match: match.group(2),\\n)\\n\\n# replacing ||| with space\\npersonality_data[\\\"tag_posts\\\"] = [\\n    post for post in personality_data[\\\"tag_posts\\\"].str.split(\\\"\\\\|\\\\|\\\\|\\\")\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating tag_posts column that will have each post as a separate list in a row. tag_posts will be a list of 50 lists.\n",
    "\n",
    "# replacing urls with domain name\n",
    "personality_data[\"tag_posts\"] = personality_data[\"posts\"].str.replace(\n",
    "    re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "    lambda match: match.group(2),\n",
    ")\n",
    "\n",
    "# replacing ||| with space\n",
    "personality_data[\"tag_posts\"] = [\n",
    "    post for post in personality_data[\"tag_posts\"].str.split(\"\\|\\|\\|\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CAUTION - The next step i.e. Parts of speech tagging for each word will take SUPER LONG !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Time: 1075.6390972137451 seconds\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# parts of speech tagging for each word\\nt = time.time()\\n\\npersonality_data['tagged_words'] = personality_data['tag_posts'].apply(\\n    lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x])\\n\\nprint(f\\\"POS Tagging Time: {time.time() - t} seconds\\\")\";\n",
       "                var nbb_formatted_code = \"# parts of speech tagging for each word\\nt = time.time()\\n\\npersonality_data[\\\"tagged_words\\\"] = personality_data[\\\"tag_posts\\\"].apply(\\n    lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\\n)\\n\\nprint(f\\\"POS Tagging Time: {time.time() - t} seconds\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parts of speech tagging for each word\n",
    "t = time.time()\n",
    "\n",
    "personality_data[\"tagged_words\"] = personality_data[\"tag_posts\"].apply(\n",
    "    lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\n",
    ")\n",
    "\n",
    "print(f\"POS Tagging Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"# creating list of unique POS tags\\ntag_set = set()\\n\\nfor i, data in personality_data[\\\"tagged_words\\\"].iteritems():\\n    for tup in data[0]:\\n        tag_set.add(tup[1])\\n\\ntag_list = list(tag_set)\";\n",
       "                var nbb_formatted_code = \"# creating list of unique POS tags\\ntag_set = set()\\n\\nfor i, data in personality_data[\\\"tagged_words\\\"].iteritems():\\n    for tup in data[0]:\\n        tag_set.add(tup[1])\\n\\ntag_list = list(tag_set)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating list of unique POS tags\n",
    "tag_set = set()\n",
    "\n",
    "for i, data in personality_data[\"tagged_words\"].iteritems():\n",
    "    for tup in data[0]:\n",
    "        tag_set.add(tup[1])\n",
    "\n",
    "tag_list = list(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Stats Time: 144.66406536102295 seconds\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# calculating mean and standard deviation of pos tags for each user\\nt = time.time()\\n\\ndef pos_cat(x, tag):\\n    return [len([y for y in line if y[1] == tag]) for line in x]\\n\\n\\nfor col in tag_list:\\n    personality_data[\\\"POS_\\\" + col + \\\"_mean\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n        lambda x: np.mean(pos_cat(x, col))\\n    )\\n    personality_data[\\\"POS_\\\" + col + \\\"_std\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n        lambda x: np.std(pos_cat(x, col))\\n    )\\n    \\nprint(f\\\"POS Stats Time: {time.time() - t} seconds\\\")\";\n",
       "                var nbb_formatted_code = \"# calculating mean and standard deviation of pos tags for each user\\nt = time.time()\\n\\n\\ndef pos_cat(x, tag):\\n    return [len([y for y in line if y[1] == tag]) for line in x]\\n\\n\\nfor col in tag_list:\\n    personality_data[\\\"POS_\\\" + col + \\\"_mean\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n        lambda x: np.mean(pos_cat(x, col))\\n    )\\n    personality_data[\\\"POS_\\\" + col + \\\"_std\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n        lambda x: np.std(pos_cat(x, col))\\n    )\\n\\nprint(f\\\"POS Stats Time: {time.time() - t} seconds\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculating mean and standard deviation of pos tags for each user\n",
    "t = time.time()\n",
    "\n",
    "\n",
    "def pos_cat(x, tag):\n",
    "    return [len([y for y in line if y[1] == tag]) for line in x]\n",
    "\n",
    "\n",
    "for col in tag_list:\n",
    "    personality_data[\"POS_\" + col + \"_mean\"] = personality_data[\"tagged_words\"].apply(\n",
    "        lambda x: np.mean(pos_cat(x, col))\n",
    "    )\n",
    "    personality_data[\"POS_\" + col + \"_std\"] = personality_data[\"tagged_words\"].apply(\n",
    "        lambda x: np.std(pos_cat(x, col))\n",
    "    )\n",
    "\n",
    "print(f\"POS Stats Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# grouping pos tags based on stanford list\\ntags_dict = {\\n    \\\"ADJ\\\": [\\\"JJ\\\", \\\"JJR\\\", \\\"JJS\\\"],\\n    \\\"ADP\\\": [\\\"EX\\\", \\\"TO\\\"],\\n    \\\"ADV\\\": [\\\"RB\\\", \\\"RBR\\\", \\\"RBS\\\", \\\"WRB\\\"],\\n    \\\"CONJ\\\": [\\\"CC\\\", \\\"IN\\\"],\\n    \\\"DET\\\": [\\\"DT\\\", \\\"PDT\\\", \\\"WDT\\\"],\\n    \\\"NOUN\\\": [\\\"NN\\\", \\\"NNS\\\", \\\"NNP\\\", \\\"NNPS\\\"],\\n    \\\"NUM\\\": [\\\"CD\\\"],\\n    \\\"PRT\\\": [\\\"RP\\\"],\\n    \\\"PRON\\\": [\\\"PRP\\\", \\\"PRP$\\\", \\\"WP\\\", \\\"WP$\\\"],\\n    \\\"VERB\\\": [\\\"MD\\\", \\\"VB\\\", \\\"VBD\\\", \\\"VBG\\\", \\\"VBN\\\", \\\"VBP\\\", \\\"VBZ\\\"],\\n    \\\".\\\": [\\\"#\\\", \\\"$\\\", \\\"''\\\", \\\"(\\\", \\\")\\\", \\\",\\\", \\\".\\\", \\\":\\\"],\\n    \\\"X\\\": [\\\"FW\\\", \\\"LS\\\", \\\"UH\\\"],\\n}\";\n",
       "                var nbb_formatted_code = \"# grouping pos tags based on stanford list\\ntags_dict = {\\n    \\\"ADJ\\\": [\\\"JJ\\\", \\\"JJR\\\", \\\"JJS\\\"],\\n    \\\"ADP\\\": [\\\"EX\\\", \\\"TO\\\"],\\n    \\\"ADV\\\": [\\\"RB\\\", \\\"RBR\\\", \\\"RBS\\\", \\\"WRB\\\"],\\n    \\\"CONJ\\\": [\\\"CC\\\", \\\"IN\\\"],\\n    \\\"DET\\\": [\\\"DT\\\", \\\"PDT\\\", \\\"WDT\\\"],\\n    \\\"NOUN\\\": [\\\"NN\\\", \\\"NNS\\\", \\\"NNP\\\", \\\"NNPS\\\"],\\n    \\\"NUM\\\": [\\\"CD\\\"],\\n    \\\"PRT\\\": [\\\"RP\\\"],\\n    \\\"PRON\\\": [\\\"PRP\\\", \\\"PRP$\\\", \\\"WP\\\", \\\"WP$\\\"],\\n    \\\"VERB\\\": [\\\"MD\\\", \\\"VB\\\", \\\"VBD\\\", \\\"VBG\\\", \\\"VBN\\\", \\\"VBP\\\", \\\"VBZ\\\"],\\n    \\\".\\\": [\\\"#\\\", \\\"$\\\", \\\"''\\\", \\\"(\\\", \\\")\\\", \\\",\\\", \\\".\\\", \\\":\\\"],\\n    \\\"X\\\": [\\\"FW\\\", \\\"LS\\\", \\\"UH\\\"],\\n}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grouping pos tags based on stanford list\n",
    "tags_dict = {\n",
    "    \"ADJ\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "    \"ADP\": [\"EX\", \"TO\"],\n",
    "    \"ADV\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
    "    \"CONJ\": [\"CC\", \"IN\"],\n",
    "    \"DET\": [\"DT\", \"PDT\", \"WDT\"],\n",
    "    \"NOUN\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "    \"NUM\": [\"CD\"],\n",
    "    \"PRT\": [\"RP\"],\n",
    "    \"PRON\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
    "    \"VERB\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "    \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
    "    \"X\": [\"FW\", \"LS\", \"UH\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford POS Stats Time: 80.72707939147949 seconds\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# Stanford POS tag stats\\nt = time.time()\\n\\n\\ndef stanford_tag(x, tag):\\n    tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\\n    return tags_list\\n\\n\\nfor col in tags_dict.keys():\\n    personality_data[\\\"S_\\\" + col + \\\"_med\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n        lambda x: np.median(stanford_tag(x, col))\\n    )\\n    personality_data[\\\"S_\\\" + col + \\\"_std\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n        lambda x: np.std(stanford_tag(x, col))\\n    )\\n\\nprint(f\\\"Stanford POS Stats Time: {time.time() - t} seconds\\\")\";\n",
       "                var nbb_formatted_code = \"# Stanford POS tag stats\\nt = time.time()\\n\\n\\ndef stanford_tag(x, tag):\\n    tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\\n    return tags_list\\n\\n\\nfor col in tags_dict.keys():\\n    personality_data[\\\"S_\\\" + col + \\\"_med\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n        lambda x: np.median(stanford_tag(x, col))\\n    )\\n    personality_data[\\\"S_\\\" + col + \\\"_std\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n        lambda x: np.std(stanford_tag(x, col))\\n    )\\n\\nprint(f\\\"Stanford POS Stats Time: {time.time() - t} seconds\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stanford POS tag stats\n",
    "t = time.time()\n",
    "\n",
    "def stanford_tag(x, tag):\n",
    "    tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\n",
    "    return tags_list\n",
    "\n",
    "\n",
    "for col in tags_dict.keys():\n",
    "    personality_data[\"S_\" + col + \"_med\"] = personality_data[\"tagged_words\"].apply(\n",
    "        lambda x: np.median(stanford_tag(x, col))\n",
    "    )\n",
    "    personality_data[\"S_\" + col + \"_std\"] = personality_data[\"tagged_words\"].apply(\n",
    "        lambda x: np.std(stanford_tag(x, col))\n",
    "    )\n",
    "\n",
    "print(f\"Stanford POS Stats Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>is_Extrovert</th>\n",
       "      <th>is_Sensing</th>\n",
       "      <th>is_Thinking</th>\n",
       "      <th>is_Judging</th>\n",
       "      <th>posts</th>\n",
       "      <th>clean_posts</th>\n",
       "      <th>compound_sentiment</th>\n",
       "      <th>pos_sentiment</th>\n",
       "      <th>neg_sentiment</th>\n",
       "      <th>...</th>\n",
       "      <th>S_PRT_med</th>\n",
       "      <th>S_PRT_std</th>\n",
       "      <th>S_PRON_med</th>\n",
       "      <th>S_PRON_std</th>\n",
       "      <th>S_VERB_med</th>\n",
       "      <th>S_VERB_std</th>\n",
       "      <th>S_._med</th>\n",
       "      <th>S_._std</th>\n",
       "      <th>S_X_med</th>\n",
       "      <th>S_X_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>youtube    moment  youtube  sportscenter top t...</td>\n",
       "      <td>0.997599</td>\n",
       "      <td>0.352861</td>\n",
       "      <td>0.292958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.719016</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.904030</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.911538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>im finding lack post alarming sex boring posit...</td>\n",
       "      <td>0.999250</td>\n",
       "      <td>0.361035</td>\n",
       "      <td>0.349296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.362433</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.036079</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.768878</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.995551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>good one     youtube course say know thats ble...</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.399183</td>\n",
       "      <td>0.315493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.206080</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.091252</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.347673</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.468968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>dear    enjoyed conversation day  esoteric gab...</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.311989</td>\n",
       "      <td>0.259155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.199826</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.547381</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.856587</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.005093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.199826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>youre fired thats another silly misconception ...</td>\n",
       "      <td>0.974042</td>\n",
       "      <td>0.318801</td>\n",
       "      <td>0.546479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284583</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.122891</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.964285</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.339568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.206080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  is_Extrovert  is_Sensing  is_Thinking  is_Judging  \\\n",
       "0  INFJ             0           0            0           1   \n",
       "1  ENTP             1           0            1           0   \n",
       "2  INTP             0           0            1           0   \n",
       "3  INTJ             0           0            1           1   \n",
       "4  ENTJ             1           0            1           1   \n",
       "\n",
       "                                               posts  \\\n",
       "0  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  'I'm finding the lack of me in these posts ver...   \n",
       "2  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                         clean_posts  compound_sentiment  \\\n",
       "0  youtube    moment  youtube  sportscenter top t...            0.997599   \n",
       "1  im finding lack post alarming sex boring posit...            0.999250   \n",
       "2  good one     youtube course say know thats ble...            0.999300   \n",
       "3  dear    enjoyed conversation day  esoteric gab...            0.999100   \n",
       "4  youre fired thats another silly misconception ...            0.974042   \n",
       "\n",
       "   pos_sentiment  neg_sentiment  ...  S_PRT_med S_PRT_std S_PRON_med  \\\n",
       "0       0.352861       0.292958  ...        0.0  0.235294        1.0   \n",
       "1       0.361035       0.349296  ...        0.0  0.362433        4.0   \n",
       "2       0.399183       0.315493  ...        0.0  0.206080        2.0   \n",
       "3       0.311989       0.259155  ...        0.0  0.199826        3.0   \n",
       "4       0.318801       0.546479  ...        0.0  0.284583        3.0   \n",
       "\n",
       "   S_PRON_std  S_VERB_med  S_VERB_std  S_._med   S_._std  S_X_med   S_X_std  \n",
       "0    1.719016         2.0    2.904030      2.5  1.911538      0.0  0.000000  \n",
       "1    3.036079         6.0    3.768878      4.0  1.995551      0.0  0.147406  \n",
       "2    2.091252         4.0    3.347673      3.0  2.468968      0.0  0.284583  \n",
       "3    2.547381         5.0    3.856587      3.0  2.005093      0.0  0.199826  \n",
       "4    2.122891         5.0    3.964285      3.0  2.339568      0.0  0.206080  \n",
       "\n",
       "[5 rows x 127 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# a quick look at the data\\npersonality_data.head()\";\n",
       "                var nbb_formatted_code = \"# a quick look at the data\\npersonality_data.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a quick look at the data\n",
    "personality_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"# Sentiment scoring & POS Tagging took long. So saving the scored & tagged file to save time in the next step.\\npersonality_data.to_csv(\\\"data_ekta/clean_data_2.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"# Sentiment scoring & POS Tagging took long. So saving the scored & tagged file to save time in the next step.\\npersonality_data.to_csv(\\\"data_ekta/clean_data_2.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sentiment scoring & POS Tagging took long. So saving the scored & tagged file to save time in the next step.\n",
    "personality_data.to_csv(\"data_ekta/clean_data_2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project3",
   "language": "python",
   "name": "project3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
