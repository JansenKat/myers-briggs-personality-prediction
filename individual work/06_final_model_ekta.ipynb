{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (MBTI) Myers-Briggs Personality Type Prediction\n",
    "\n",
    "* Extroversion vs. Introversion\n",
    "    * I - 0\n",
    "    * E - 1 \n",
    "    \n",
    "* Sensing vs. Intuition \n",
    "    * N - 0 \n",
    "    * S - 1\n",
    "    \n",
    "* Thinking vs. Feeling\n",
    "    * F - 0\n",
    "    * T - 1\n",
    "    \n",
    "* Judging vs. Perceiving\n",
    "    * P - 0\n",
    "    * J - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\eshom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# importing dependencies here\\nimport numpy as np\\nimport pandas as pd\\n\\n# visualizations\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# feature engineering\\nimport re\\nimport nltk\\nfrom nltk.corpus import stopwords\\n\\nnltk.download(\\\"stopwords\\\")\\nnltk.download(\\\"punkt\\\")\\nnltk.download(\\\"averaged_perceptron_tagger\\\")\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\nfrom nltk.stem import WordNetLemmatizer\\n\\nnltk.download(\\\"wordnet\\\")\\nnltk.download(\\\"vader_lexicon\\\")\\n\\n# sentiment scoring\\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n\\n# scikit\\n# vectorization\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# scaling to handle negative values (for Naive Bayes)\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# data stratifying and splitting\\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\\nfrom sklearn.model_selection import train_test_split\\n\\n# algorithms/models\\n# from sklearn.pipeline import make_pipeline\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\nfrom imblearn.ensemble import BalancedRandomForestClassifier\\nfrom sklearn.preprocessing import Normalizer\\nfrom imblearn.pipeline import make_pipeline as imb_make_pipeline\\nfrom imblearn.under_sampling import RandomUnderSampler\\nfrom imblearn.over_sampling import RandomOverSampler\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.over_sampling import ADASYN\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.metrics import confusion_matrix\\n\\n# model performance evaluation and selection\\nfrom sklearn.metrics import (\\n    classification_report,\\n    f1_score,\\n    accuracy_score,\\n    roc_auc_score,\\n)\\n\\n# performance check\\nimport time\\n\\nfrom joblib import load\\n\\n# code formatter\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# importing dependencies here\\nimport numpy as np\\nimport pandas as pd\\n\\n# visualizations\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# feature engineering\\nimport re\\nimport nltk\\nfrom nltk.corpus import stopwords\\n\\nnltk.download(\\\"stopwords\\\")\\nnltk.download(\\\"punkt\\\")\\nnltk.download(\\\"averaged_perceptron_tagger\\\")\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\nfrom nltk.stem import WordNetLemmatizer\\n\\nnltk.download(\\\"wordnet\\\")\\nnltk.download(\\\"vader_lexicon\\\")\\n\\n# sentiment scoring\\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n\\n# scikit\\n# vectorization\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# scaling to handle negative values (for Naive Bayes)\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# data stratifying and splitting\\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\\nfrom sklearn.model_selection import train_test_split\\n\\n# algorithms/models\\n# from sklearn.pipeline import make_pipeline\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\nfrom imblearn.ensemble import BalancedRandomForestClassifier\\nfrom sklearn.preprocessing import Normalizer\\nfrom imblearn.pipeline import make_pipeline as imb_make_pipeline\\nfrom imblearn.under_sampling import RandomUnderSampler\\nfrom imblearn.over_sampling import RandomOverSampler\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.over_sampling import ADASYN\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.metrics import confusion_matrix\\n\\n# model performance evaluation and selection\\nfrom sklearn.metrics import (\\n    classification_report,\\n    f1_score,\\n    accuracy_score,\\n    roc_auc_score,\\n)\\n\\n# performance check\\nimport time\\n\\nfrom joblib import load\\n\\n# code formatter\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing dependencies here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# feature engineering\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "# sentiment scoring\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# scikit\n",
    "# vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# scaling to handle negative values (for Naive Bayes)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# data stratifying and splitting\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# algorithms/models\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from imblearn.pipeline import make_pipeline as imb_make_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# model performance evaluation and selection\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# performance check\n",
    "import time\n",
    "\n",
    "from joblib import load\n",
    "\n",
    "# code formatter\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# reading the test dataset\\ndf = pd.read_csv(\\\"../data/test_data.csv\\\")\";\n",
       "                var nbb_formatted_code = \"# reading the test dataset\\ndf = pd.read_csv(\\\"../data/test_data.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reading the test dataset\n",
    "df = pd.read_csv(\"../data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>ESTP</td>\n",
       "      <td>My thoughts and prayers are with the @USMC cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>ENFJ</td>\n",
       "      <td>Happy Hanukkah! Over these eight nights, we dr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name  type                                              posts\n",
       "0  Donald Trump  ESTP  My thoughts and prayers are with the @USMC cre...\n",
       "1  Barack Obama  ENFJ  Happy Hanukkah! Over these eight nights, we dr..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# checking top records\\ndf.head(2)\";\n",
       "                var nbb_formatted_code = \"# checking top records\\ndf.head(2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking top records\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"def categorize_types(personality_data):\\n\\n    personality_data[\\\"is_Extrovert\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[0] == \\\"E\\\" else 0\\n    )\\n    personality_data[\\\"is_Sensing\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[1] == \\\"S\\\" else 0\\n    )\\n    personality_data[\\\"is_Thinking\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[2] == \\\"T\\\" else 0\\n    )\\n    personality_data[\\\"is_Judging\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[3] == \\\"J\\\" else 0\\n    )\\n\\n    # rearranging the dataframe columns\\n    personality_data = personality_data[\\n        [\\\"type\\\", \\\"is_Extrovert\\\", \\\"is_Sensing\\\", \\\"is_Thinking\\\", \\\"is_Judging\\\", \\\"posts\\\"]\\n    ]\\n\\n\\n#######################################################################################################3\\n\\n\\ndef clean_posts(personality_data):\\n\\n    # converting posts into lower case\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"posts\\\"].str.lower()\\n\\n    # replacing ||| with space\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\|\\\\|\\\\|\\\"), \\\"\\\"\\n    )\\n\\n    # replacing urls with domain name\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        lambda match: match.group(2),\\n    )\\n\\n    # dropping emails\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\S+@\\\\S+\\\"), \\\"\\\"\\n    )\\n\\n    # dropping punctuations\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"[^a-z\\\\s]\\\"), \\\"\\\"\\n    )\\n\\n    # dropping MBTIs mentioned in the posts. There are quite a few mention of these types in these posts.\\n    mbti = personality_data[\\\"type\\\"].unique()\\n    for type_word in mbti:\\n        personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n            type_word.lower(), \\\"\\\"\\n        )\\n\\n    # tag_posts will be a list of 50 lists. need it for word stats (per post for each user)\\n    # replacing urls with domain name\\n    personality_data[\\\"tag_posts\\\"] = personality_data[\\\"posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        lambda match: match.group(2),\\n    )\\n    # replacing ||| with space\\n    personality_data[\\\"tag_posts\\\"] = [\\n        post for post in personality_data[\\\"tag_posts\\\"].str.split(\\\"\\\\|\\\\|\\\\|\\\")\\n    ]\\n\\n\\n#################################################################################################################\\n\\n\\ndef sentiment_score(personality_data):\\n\\n    analyzer = SentimentIntensityAnalyzer()\\n\\n    nlp_sentiment_score = []\\n\\n    for post in personality_data[\\\"clean_posts\\\"]:\\n        score = analyzer.polarity_scores(post)[\\\"compound\\\"]\\n        nlp_sentiment_score.append(score)\\n\\n    personality_data[\\\"compound_sentiment\\\"] = nlp_sentiment_score\\n\\n\\n###############################################################################################################\\n\\n\\ndef pos_tagging(personality_data):\\n\\n    personality_data[\\\"tagged_words\\\"] = personality_data[\\\"tag_posts\\\"].apply(\\n        lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\\n    )\\n\\n    # grouping pos tags based on stanford list\\n    tags_dict = {\\n        \\\"ADJ\\\": [\\\"JJ\\\", \\\"JJR\\\", \\\"JJS\\\"],\\n        \\\"ADP\\\": [\\\"EX\\\", \\\"TO\\\"],\\n        \\\"ADV\\\": [\\\"RB\\\", \\\"RBR\\\", \\\"RBS\\\", \\\"WRB\\\"],\\n        \\\"CONJ\\\": [\\\"CC\\\", \\\"IN\\\"],\\n        \\\"DET\\\": [\\\"DT\\\", \\\"PDT\\\", \\\"WDT\\\"],\\n        \\\"NOUN\\\": [\\\"NN\\\", \\\"NNS\\\", \\\"NNP\\\", \\\"NNPS\\\"],\\n        \\\"NUM\\\": [\\\"CD\\\"],\\n        \\\"PRT\\\": [\\\"RP\\\"],\\n        \\\"PRON\\\": [\\\"PRP\\\", \\\"PRP$\\\", \\\"WP\\\", \\\"WP$\\\"],\\n        \\\"VERB\\\": [\\\"MD\\\", \\\"VB\\\", \\\"VBD\\\", \\\"VBG\\\", \\\"VBN\\\", \\\"VBP\\\", \\\"VBZ\\\"],\\n        \\\".\\\": [\\\"#\\\", \\\"$\\\", \\\"''\\\", \\\"(\\\", \\\")\\\", \\\",\\\", \\\".\\\", \\\":\\\"],\\n        \\\"X\\\": [\\\"FW\\\", \\\"LS\\\", \\\"UH\\\"],\\n    }\\n\\n    def stanford_tag(x, tag):\\n        tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\\n        return tags_list\\n\\n    for col in tags_dict.keys():\\n        personality_data[\\\"S_\\\" + col + \\\"_med\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n            lambda x: np.median(stanford_tag(x, col))\\n        )\\n        personality_data[\\\"S_\\\" + col + \\\"_std\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n            lambda x: np.std(stanford_tag(x, col))\\n        )\\n\\n\\n###############################################################################################################\\n\\n\\ndef get_counts(personality_data):\\n    def unique_words(s):\\n        unique = set(s.split(\\\" \\\"))\\n        return len(unique)\\n\\n    def emojis(post):\\n        # does not include emojis made purely from symbols, only :word:\\n        emoji_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                if e.count(\\\":\\\") == 2:\\n                    emoji_count += 1\\n        return emoji_count\\n\\n    def colons(post):\\n        # Includes colons used in emojis\\n        colon_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                colon_count += e.count(\\\":\\\")\\n        return colon_count\\n\\n    personality_data[\\\"qm\\\"] = personality_data[\\\"posts\\\"].apply(lambda s: s.count(\\\"?\\\"))\\n    personality_data[\\\"em\\\"] = personality_data[\\\"posts\\\"].apply(lambda s: s.count(\\\"!\\\"))\\n    personality_data[\\\"colons\\\"] = personality_data[\\\"posts\\\"].apply(colons)\\n    personality_data[\\\"emojis\\\"] = personality_data[\\\"posts\\\"].apply(emojis)\\n    personality_data[\\\"word_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\" \\\") + 1\\n    )\\n    personality_data[\\\"unique_words\\\"] = personality_data[\\\"posts\\\"].apply(unique_words)\\n    personality_data[\\\"avg_word_ct\\\"] = personality_data[\\\"word_count\\\"].apply(\\n        lambda s: s / 50\\n    )\\n    personality_data[\\\"post_length_var\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda x: np.var([len(post.split()) for post in x.split(\\\"|||\\\")])\\n    )\\n    #     personality_data[\\\"med_char\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n    #         lambda x: np.median([len(i) for i in x]))\\n    #     personality_data[\\\"med_word\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n    #         lambda x: np.median([len(i.split()) for i in x]))\\n    personality_data[\\\"upper\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda x: len([x for x in x.split() if x.isupper()])\\n    )\\n    personality_data[\\\"link_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\"http\\\")\\n    )\\n    ellipses_count = [\\n        len(re.findall(r\\\"\\\\.\\\\.\\\\.\\\\ \\\", posts)) for posts in personality_data[\\\"posts\\\"]\\n    ]\\n    personality_data[\\\"ellipses\\\"] = ellipses_count\\n    personality_data[\\\"img_count\\\"] = [\\n        len(re.findall(r\\\"(\\\\.jpg)|(\\\\.jpeg)|(\\\\.gif)|(\\\\.png)\\\", post))\\n        for post in personality_data[\\\"posts\\\"]\\n    ]\\n\\n\\n#################################################################################################################\\n\\n\\ndef vectorize(personality_data):\\n\\n    tfidf_vectorizer = TfidfVectorizer(\\n        min_df=0.05, max_df=0.85, analyzer=\\\"word\\\", ngram_range=(1, 2), max_features=1500\\n    )\\n    tfidf_words = tfidf_vectorizer.fit_transform(personality_data[\\\"clean_posts\\\"])\\n\\n    tfidf_vectorized_data = pd.DataFrame(\\n        data=tfidf_words.toarray(), columns=tfidf_vectorizer.get_feature_names()\\n    )\\n    return tfidf_vectorized_data\";\n",
       "                var nbb_formatted_code = \"def categorize_types(personality_data):\\n\\n    personality_data[\\\"is_Extrovert\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[0] == \\\"E\\\" else 0\\n    )\\n    personality_data[\\\"is_Sensing\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[1] == \\\"S\\\" else 0\\n    )\\n    personality_data[\\\"is_Thinking\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[2] == \\\"T\\\" else 0\\n    )\\n    personality_data[\\\"is_Judging\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[3] == \\\"J\\\" else 0\\n    )\\n\\n    # rearranging the dataframe columns\\n    personality_data = personality_data[\\n        [\\\"type\\\", \\\"is_Extrovert\\\", \\\"is_Sensing\\\", \\\"is_Thinking\\\", \\\"is_Judging\\\", \\\"posts\\\"]\\n    ]\\n\\n\\n#######################################################################################################3\\n\\n\\ndef clean_posts(personality_data):\\n\\n    # converting posts into lower case\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"posts\\\"].str.lower()\\n\\n    # replacing ||| with space\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\|\\\\|\\\\|\\\"), \\\"\\\"\\n    )\\n\\n    # replacing urls with domain name\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        lambda match: match.group(2),\\n    )\\n\\n    # dropping emails\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\S+@\\\\S+\\\"), \\\"\\\"\\n    )\\n\\n    # dropping punctuations\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"[^a-z\\\\s]\\\"), \\\"\\\"\\n    )\\n\\n    # dropping MBTIs mentioned in the posts. There are quite a few mention of these types in these posts.\\n    mbti = personality_data[\\\"type\\\"].unique()\\n    for type_word in mbti:\\n        personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n            type_word.lower(), \\\"\\\"\\n        )\\n\\n    # tag_posts will be a list of 50 lists. need it for word stats (per post for each user)\\n    # replacing urls with domain name\\n    personality_data[\\\"tag_posts\\\"] = personality_data[\\\"posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        lambda match: match.group(2),\\n    )\\n    # replacing ||| with space\\n    personality_data[\\\"tag_posts\\\"] = [\\n        post for post in personality_data[\\\"tag_posts\\\"].str.split(\\\"\\\\|\\\\|\\\\|\\\")\\n    ]\\n\\n\\n#################################################################################################################\\n\\n\\ndef sentiment_score(personality_data):\\n\\n    analyzer = SentimentIntensityAnalyzer()\\n\\n    nlp_sentiment_score = []\\n\\n    for post in personality_data[\\\"clean_posts\\\"]:\\n        score = analyzer.polarity_scores(post)[\\\"compound\\\"]\\n        nlp_sentiment_score.append(score)\\n\\n    personality_data[\\\"compound_sentiment\\\"] = nlp_sentiment_score\\n\\n\\n###############################################################################################################\\n\\n\\ndef pos_tagging(personality_data):\\n\\n    personality_data[\\\"tagged_words\\\"] = personality_data[\\\"tag_posts\\\"].apply(\\n        lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\\n    )\\n\\n    # grouping pos tags based on stanford list\\n    tags_dict = {\\n        \\\"ADJ\\\": [\\\"JJ\\\", \\\"JJR\\\", \\\"JJS\\\"],\\n        \\\"ADP\\\": [\\\"EX\\\", \\\"TO\\\"],\\n        \\\"ADV\\\": [\\\"RB\\\", \\\"RBR\\\", \\\"RBS\\\", \\\"WRB\\\"],\\n        \\\"CONJ\\\": [\\\"CC\\\", \\\"IN\\\"],\\n        \\\"DET\\\": [\\\"DT\\\", \\\"PDT\\\", \\\"WDT\\\"],\\n        \\\"NOUN\\\": [\\\"NN\\\", \\\"NNS\\\", \\\"NNP\\\", \\\"NNPS\\\"],\\n        \\\"NUM\\\": [\\\"CD\\\"],\\n        \\\"PRT\\\": [\\\"RP\\\"],\\n        \\\"PRON\\\": [\\\"PRP\\\", \\\"PRP$\\\", \\\"WP\\\", \\\"WP$\\\"],\\n        \\\"VERB\\\": [\\\"MD\\\", \\\"VB\\\", \\\"VBD\\\", \\\"VBG\\\", \\\"VBN\\\", \\\"VBP\\\", \\\"VBZ\\\"],\\n        \\\".\\\": [\\\"#\\\", \\\"$\\\", \\\"''\\\", \\\"(\\\", \\\")\\\", \\\",\\\", \\\".\\\", \\\":\\\"],\\n        \\\"X\\\": [\\\"FW\\\", \\\"LS\\\", \\\"UH\\\"],\\n    }\\n\\n    def stanford_tag(x, tag):\\n        tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\\n        return tags_list\\n\\n    for col in tags_dict.keys():\\n        personality_data[\\\"S_\\\" + col + \\\"_med\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n            lambda x: np.median(stanford_tag(x, col))\\n        )\\n        personality_data[\\\"S_\\\" + col + \\\"_std\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n            lambda x: np.std(stanford_tag(x, col))\\n        )\\n\\n\\n###############################################################################################################\\n\\n\\ndef get_counts(personality_data):\\n    def unique_words(s):\\n        unique = set(s.split(\\\" \\\"))\\n        return len(unique)\\n\\n    def emojis(post):\\n        # does not include emojis made purely from symbols, only :word:\\n        emoji_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                if e.count(\\\":\\\") == 2:\\n                    emoji_count += 1\\n        return emoji_count\\n\\n    def colons(post):\\n        # Includes colons used in emojis\\n        colon_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                colon_count += e.count(\\\":\\\")\\n        return colon_count\\n\\n    personality_data[\\\"qm\\\"] = personality_data[\\\"posts\\\"].apply(lambda s: s.count(\\\"?\\\"))\\n    personality_data[\\\"em\\\"] = personality_data[\\\"posts\\\"].apply(lambda s: s.count(\\\"!\\\"))\\n    personality_data[\\\"colons\\\"] = personality_data[\\\"posts\\\"].apply(colons)\\n    personality_data[\\\"emojis\\\"] = personality_data[\\\"posts\\\"].apply(emojis)\\n    personality_data[\\\"word_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\" \\\") + 1\\n    )\\n    personality_data[\\\"unique_words\\\"] = personality_data[\\\"posts\\\"].apply(unique_words)\\n    personality_data[\\\"avg_word_ct\\\"] = personality_data[\\\"word_count\\\"].apply(\\n        lambda s: s / 50\\n    )\\n    personality_data[\\\"post_length_var\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda x: np.var([len(post.split()) for post in x.split(\\\"|||\\\")])\\n    )\\n    #     personality_data[\\\"med_char\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n    #         lambda x: np.median([len(i) for i in x]))\\n    #     personality_data[\\\"med_word\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n    #         lambda x: np.median([len(i.split()) for i in x]))\\n    personality_data[\\\"upper\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda x: len([x for x in x.split() if x.isupper()])\\n    )\\n    personality_data[\\\"link_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\"http\\\")\\n    )\\n    ellipses_count = [\\n        len(re.findall(r\\\"\\\\.\\\\.\\\\.\\\\ \\\", posts)) for posts in personality_data[\\\"posts\\\"]\\n    ]\\n    personality_data[\\\"ellipses\\\"] = ellipses_count\\n    personality_data[\\\"img_count\\\"] = [\\n        len(re.findall(r\\\"(\\\\.jpg)|(\\\\.jpeg)|(\\\\.gif)|(\\\\.png)\\\", post))\\n        for post in personality_data[\\\"posts\\\"]\\n    ]\\n\\n\\n#################################################################################################################\\n\\n\\ndef vectorize(personality_data):\\n\\n    tfidf_vectorizer = TfidfVectorizer(\\n        min_df=0.05, max_df=0.85, analyzer=\\\"word\\\", ngram_range=(1, 2), max_features=1500\\n    )\\n    tfidf_words = tfidf_vectorizer.fit_transform(personality_data[\\\"clean_posts\\\"])\\n\\n    tfidf_vectorized_data = pd.DataFrame(\\n        data=tfidf_words.toarray(), columns=tfidf_vectorizer.get_feature_names()\\n    )\\n    return tfidf_vectorized_data\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def categorize_types(personality_data):\n",
    "\n",
    "    personality_data[\"is_Extrovert\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[0] == \"E\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Sensing\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[1] == \"S\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Thinking\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[2] == \"T\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Judging\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[3] == \"J\" else 0\n",
    "    )\n",
    "\n",
    "    # rearranging the dataframe columns\n",
    "    personality_data = personality_data[\n",
    "        [\"type\", \"is_Extrovert\", \"is_Sensing\", \"is_Thinking\", \"is_Judging\", \"posts\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "#######################################################################################################3\n",
    "\n",
    "\n",
    "def clean_posts(personality_data):\n",
    "\n",
    "    # converting posts into lower case\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"posts\"].str.lower()\n",
    "\n",
    "    # replacing ||| with space\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"\\|\\|\\|\"), \"\"\n",
    "    )\n",
    "\n",
    "    # replacing urls with domain name\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "        lambda match: match.group(2),\n",
    "    )\n",
    "\n",
    "    # dropping emails\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"\\S+@\\S+\"), \"\"\n",
    "    )\n",
    "\n",
    "    # dropping punctuations\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"[^a-z\\s]\"), \"\"\n",
    "    )\n",
    "\n",
    "    # dropping MBTIs mentioned in the posts. There are quite a few mention of these types in these posts.\n",
    "    mbti = personality_data[\"type\"].unique()\n",
    "    for type_word in mbti:\n",
    "        personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "            type_word.lower(), \"\"\n",
    "        )\n",
    "\n",
    "    # tag_posts will be a list of 50 lists. need it for word stats (per post for each user)\n",
    "    # replacing urls with domain name\n",
    "    personality_data[\"tag_posts\"] = personality_data[\"posts\"].str.replace(\n",
    "        re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "        lambda match: match.group(2),\n",
    "    )\n",
    "    # replacing ||| with space\n",
    "    personality_data[\"tag_posts\"] = [\n",
    "        post for post in personality_data[\"tag_posts\"].str.split(\"\\|\\|\\|\")\n",
    "    ]\n",
    "\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "\n",
    "def sentiment_score(personality_data):\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    nlp_sentiment_score = []\n",
    "\n",
    "    for post in personality_data[\"clean_posts\"]:\n",
    "        score = analyzer.polarity_scores(post)[\"compound\"]\n",
    "        nlp_sentiment_score.append(score)\n",
    "\n",
    "    personality_data[\"compound_sentiment\"] = nlp_sentiment_score\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "\n",
    "def pos_tagging(personality_data):\n",
    "\n",
    "    personality_data[\"tagged_words\"] = personality_data[\"tag_posts\"].apply(\n",
    "        lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\n",
    "    )\n",
    "\n",
    "    # grouping pos tags based on stanford list\n",
    "    tags_dict = {\n",
    "        \"ADJ\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "        \"ADP\": [\"EX\", \"TO\"],\n",
    "        \"ADV\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
    "        \"CONJ\": [\"CC\", \"IN\"],\n",
    "        \"DET\": [\"DT\", \"PDT\", \"WDT\"],\n",
    "        \"NOUN\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "        \"NUM\": [\"CD\"],\n",
    "        \"PRT\": [\"RP\"],\n",
    "        \"PRON\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
    "        \"VERB\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "        \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
    "        \"X\": [\"FW\", \"LS\", \"UH\"],\n",
    "    }\n",
    "\n",
    "    def stanford_tag(x, tag):\n",
    "        tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\n",
    "        return tags_list\n",
    "\n",
    "    for col in tags_dict.keys():\n",
    "        personality_data[\"S_\" + col + \"_med\"] = personality_data[\"tagged_words\"].apply(\n",
    "            lambda x: np.median(stanford_tag(x, col))\n",
    "        )\n",
    "        personality_data[\"S_\" + col + \"_std\"] = personality_data[\"tagged_words\"].apply(\n",
    "            lambda x: np.std(stanford_tag(x, col))\n",
    "        )\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "\n",
    "def get_counts(personality_data):\n",
    "    def unique_words(s):\n",
    "        unique = set(s.split(\" \"))\n",
    "        return len(unique)\n",
    "\n",
    "    def emojis(post):\n",
    "        # does not include emojis made purely from symbols, only :word:\n",
    "        emoji_count = 0\n",
    "        words = post.split()\n",
    "        for e in words:\n",
    "            if \"http\" not in e:\n",
    "                if e.count(\":\") == 2:\n",
    "                    emoji_count += 1\n",
    "        return emoji_count\n",
    "\n",
    "    def colons(post):\n",
    "        # Includes colons used in emojis\n",
    "        colon_count = 0\n",
    "        words = post.split()\n",
    "        for e in words:\n",
    "            if \"http\" not in e:\n",
    "                colon_count += e.count(\":\")\n",
    "        return colon_count\n",
    "\n",
    "    personality_data[\"qm\"] = personality_data[\"posts\"].apply(lambda s: s.count(\"?\"))\n",
    "    personality_data[\"em\"] = personality_data[\"posts\"].apply(lambda s: s.count(\"!\"))\n",
    "    personality_data[\"colons\"] = personality_data[\"posts\"].apply(colons)\n",
    "    personality_data[\"emojis\"] = personality_data[\"posts\"].apply(emojis)\n",
    "    personality_data[\"word_count\"] = personality_data[\"posts\"].apply(\n",
    "        lambda s: s.count(\" \") + 1\n",
    "    )\n",
    "    personality_data[\"unique_words\"] = personality_data[\"posts\"].apply(unique_words)\n",
    "    personality_data[\"avg_word_ct\"] = personality_data[\"word_count\"].apply(\n",
    "        lambda s: s / 50\n",
    "    )\n",
    "    personality_data[\"post_length_var\"] = personality_data[\"posts\"].apply(\n",
    "        lambda x: np.var([len(post.split()) for post in x.split(\"|||\")])\n",
    "    )\n",
    "    #     personality_data[\"med_char\"] = personality_data[\"tagged_words\"].apply(\n",
    "    #         lambda x: np.median([len(i) for i in x]))\n",
    "    #     personality_data[\"med_word\"] = personality_data[\"tagged_words\"].apply(\n",
    "    #         lambda x: np.median([len(i.split()) for i in x]))\n",
    "    personality_data[\"upper\"] = personality_data[\"posts\"].apply(\n",
    "        lambda x: len([x for x in x.split() if x.isupper()])\n",
    "    )\n",
    "    personality_data[\"link_count\"] = personality_data[\"posts\"].apply(\n",
    "        lambda s: s.count(\"http\")\n",
    "    )\n",
    "    ellipses_count = [\n",
    "        len(re.findall(r\"\\.\\.\\.\\ \", posts)) for posts in personality_data[\"posts\"]\n",
    "    ]\n",
    "    personality_data[\"ellipses\"] = ellipses_count\n",
    "    personality_data[\"img_count\"] = [\n",
    "        len(re.findall(r\"(\\.jpg)|(\\.jpeg)|(\\.gif)|(\\.png)\", post))\n",
    "        for post in personality_data[\"posts\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "\n",
    "def vectorize(personality_data):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        min_df=0.05, max_df=0.85, analyzer=\"word\", ngram_range=(1, 2), max_features=1500\n",
    "    )\n",
    "    tfidf_words = tfidf_vectorizer.fit_transform(personality_data[\"clean_posts\"])\n",
    "\n",
    "    tfidf_vectorized_data = pd.DataFrame(\n",
    "        data=tfidf_words.toarray(), columns=tfidf_vectorizer.get_feature_names()\n",
    "    )\n",
    "    return tfidf_vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def prep_data(personality_data):\\n\\n    t = time.time()\\n\\n    categorize_types(personality_data)\\n\\n    clean_posts(personality_data)\\n\\n    sentiment_score(personality_data)\\n\\n    pos_tagging(personality_data)\\n\\n    get_counts(personality_data)\\n\\n    tfidf_vectorized_data = vectorize(personality_data)\\n\\n    features = personality_data[\\n        [\\n            \\\"compound_sentiment\\\",\\n            \\\"S_ADJ_med\\\",\\n            \\\"S_ADJ_std\\\",\\n            \\\"S_ADP_med\\\",\\n            \\\"S_ADP_std\\\",\\n            \\\"S_ADV_med\\\",\\n            \\\"S_ADV_std\\\",\\n            \\\"S_CONJ_med\\\",\\n            \\\"S_CONJ_std\\\",\\n            \\\"S_DET_med\\\",\\n            \\\"S_DET_std\\\",\\n            \\\"S_NOUN_med\\\",\\n            \\\"S_NOUN_std\\\",\\n            \\\"S_NUM_med\\\",\\n            \\\"S_NUM_std\\\",\\n            \\\"S_PRT_med\\\",\\n            \\\"S_PRT_std\\\",\\n            \\\"S_PRON_med\\\",\\n            \\\"S_PRON_std\\\",\\n            \\\"S_VERB_med\\\",\\n            \\\"S_VERB_std\\\",\\n            \\\"qm\\\",\\n            \\\"em\\\",\\n            \\\"colons\\\",\\n            \\\"emojis\\\",\\n            \\\"word_count\\\",\\n            \\\"unique_words\\\",\\n            \\\"avg_word_ct\\\",\\n            \\\"post_length_var\\\",\\n            #         \\\"med_char\\\",\\n            #         \\\"med_word\\\",\\n            \\\"upper\\\",\\n            \\\"link_count\\\",\\n            \\\"ellipses\\\",\\n            \\\"img_count\\\",\\n        ]\\n    ]\\n\\n    X = pd.concat([features, tfidf_vectorized_data], axis=1)\\n    y = personality_data.iloc[:, 3:7]\\n\\n    print(f\\\"Total Preprocessing Time: {time.time()-t} seconds\\\")\\n\\n    return X, y\";\n",
       "                var nbb_formatted_code = \"def prep_data(personality_data):\\n\\n    t = time.time()\\n\\n    categorize_types(personality_data)\\n\\n    clean_posts(personality_data)\\n\\n    sentiment_score(personality_data)\\n\\n    pos_tagging(personality_data)\\n\\n    get_counts(personality_data)\\n\\n    tfidf_vectorized_data = vectorize(personality_data)\\n\\n    features = personality_data[\\n        [\\n            \\\"compound_sentiment\\\",\\n            \\\"S_ADJ_med\\\",\\n            \\\"S_ADJ_std\\\",\\n            \\\"S_ADP_med\\\",\\n            \\\"S_ADP_std\\\",\\n            \\\"S_ADV_med\\\",\\n            \\\"S_ADV_std\\\",\\n            \\\"S_CONJ_med\\\",\\n            \\\"S_CONJ_std\\\",\\n            \\\"S_DET_med\\\",\\n            \\\"S_DET_std\\\",\\n            \\\"S_NOUN_med\\\",\\n            \\\"S_NOUN_std\\\",\\n            \\\"S_NUM_med\\\",\\n            \\\"S_NUM_std\\\",\\n            \\\"S_PRT_med\\\",\\n            \\\"S_PRT_std\\\",\\n            \\\"S_PRON_med\\\",\\n            \\\"S_PRON_std\\\",\\n            \\\"S_VERB_med\\\",\\n            \\\"S_VERB_std\\\",\\n            \\\"qm\\\",\\n            \\\"em\\\",\\n            \\\"colons\\\",\\n            \\\"emojis\\\",\\n            \\\"word_count\\\",\\n            \\\"unique_words\\\",\\n            \\\"avg_word_ct\\\",\\n            \\\"post_length_var\\\",\\n            #         \\\"med_char\\\",\\n            #         \\\"med_word\\\",\\n            \\\"upper\\\",\\n            \\\"link_count\\\",\\n            \\\"ellipses\\\",\\n            \\\"img_count\\\",\\n        ]\\n    ]\\n\\n    X = pd.concat([features, tfidf_vectorized_data], axis=1)\\n    y = personality_data.iloc[:, 3:7]\\n\\n    print(f\\\"Total Preprocessing Time: {time.time()-t} seconds\\\")\\n\\n    return X, y\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prep_data(personality_data):\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    categorize_types(personality_data)\n",
    "\n",
    "    clean_posts(personality_data)\n",
    "\n",
    "    sentiment_score(personality_data)\n",
    "\n",
    "    pos_tagging(personality_data)\n",
    "\n",
    "    get_counts(personality_data)\n",
    "\n",
    "    tfidf_vectorized_data = vectorize(personality_data)\n",
    "\n",
    "    features = personality_data[\n",
    "        [\n",
    "            \"compound_sentiment\",\n",
    "            \"S_ADJ_med\",\n",
    "            \"S_ADJ_std\",\n",
    "            \"S_ADP_med\",\n",
    "            \"S_ADP_std\",\n",
    "            \"S_ADV_med\",\n",
    "            \"S_ADV_std\",\n",
    "            \"S_CONJ_med\",\n",
    "            \"S_CONJ_std\",\n",
    "            \"S_DET_med\",\n",
    "            \"S_DET_std\",\n",
    "            \"S_NOUN_med\",\n",
    "            \"S_NOUN_std\",\n",
    "            \"S_NUM_med\",\n",
    "            \"S_NUM_std\",\n",
    "            \"S_PRT_med\",\n",
    "            \"S_PRT_std\",\n",
    "            \"S_PRON_med\",\n",
    "            \"S_PRON_std\",\n",
    "            \"S_VERB_med\",\n",
    "            \"S_VERB_std\",\n",
    "            \"qm\",\n",
    "            \"em\",\n",
    "            \"colons\",\n",
    "            \"emojis\",\n",
    "            \"word_count\",\n",
    "            \"unique_words\",\n",
    "            \"avg_word_ct\",\n",
    "            \"post_length_var\",\n",
    "            #         \"med_char\",\n",
    "            #         \"med_word\",\n",
    "            \"upper\",\n",
    "            \"link_count\",\n",
    "            \"ellipses\",\n",
    "            \"img_count\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    X = pd.concat([features, tfidf_vectorized_data], axis=1)\n",
    "    y = personality_data.iloc[:, 3:7]\n",
    "\n",
    "    print(f\"Total Preprocessing Time: {time.time()-t} seconds\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"def combine_classes(y_pred1, y_pred2, y_pred3, y_pred4):\\n    \\n    combined = []\\n    for i in range(len(y_pred1)):\\n        combined.append(\\n            str(y_pred1[i]) + str(y_pred2[i]) + str(y_pred3[i]) + str(y_pred4[i])\\n        )\\n    \\n    result = trace_back(combined)\\n    return result\\n    \\n\\ndef trace_back(combined):\\n        \\n    type_list = [\\n    {\\\"0\\\": \\\"I\\\", \\\"1\\\": \\\"E\\\"},\\n    {\\\"0\\\": \\\"N\\\", \\\"1\\\": \\\"S\\\"},\\n    {\\\"0\\\": \\\"F\\\", \\\"1\\\": \\\"T\\\"},\\n    {\\\"0\\\": \\\"P\\\", \\\"1\\\": \\\"J\\\"},\\n    ]\\n\\n    result = []\\n    for num in combined:\\n        s = \\\"\\\"\\n        for i in range(len(num)):\\n            s += type_list[i][num[i]]\\n        result.append(s)\\n        \\n    return result\";\n",
       "                var nbb_formatted_code = \"def combine_classes(y_pred1, y_pred2, y_pred3, y_pred4):\\n\\n    combined = []\\n    for i in range(len(y_pred1)):\\n        combined.append(\\n            str(y_pred1[i]) + str(y_pred2[i]) + str(y_pred3[i]) + str(y_pred4[i])\\n        )\\n\\n    result = trace_back(combined)\\n    return result\\n\\n\\ndef trace_back(combined):\\n\\n    type_list = [\\n        {\\\"0\\\": \\\"I\\\", \\\"1\\\": \\\"E\\\"},\\n        {\\\"0\\\": \\\"N\\\", \\\"1\\\": \\\"S\\\"},\\n        {\\\"0\\\": \\\"F\\\", \\\"1\\\": \\\"T\\\"},\\n        {\\\"0\\\": \\\"P\\\", \\\"1\\\": \\\"J\\\"},\\n    ]\\n\\n    result = []\\n    for num in combined:\\n        s = \\\"\\\"\\n        for i in range(len(num)):\\n            s += type_list[i][num[i]]\\n        result.append(s)\\n\\n    return result\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def combine_classes(y_pred1, y_pred2, y_pred3, y_pred4):\n",
    "    \n",
    "    combined = []\n",
    "    for i in range(len(y_pred1)):\n",
    "        combined.append(\n",
    "            str(y_pred1[i]) + str(y_pred2[i]) + str(y_pred3[i]) + str(y_pred4[i])\n",
    "        )\n",
    "    \n",
    "    result = trace_back(combined)\n",
    "    return result\n",
    "    \n",
    "\n",
    "def trace_back(combined):\n",
    "        \n",
    "    type_list = [\n",
    "    {\"0\": \"I\", \"1\": \"E\"},\n",
    "    {\"0\": \"N\", \"1\": \"S\"},\n",
    "    {\"0\": \"F\", \"1\": \"T\"},\n",
    "    {\"0\": \"P\", \"1\": \"J\"},\n",
    "    ]\n",
    "\n",
    "    result = []\n",
    "    for num in combined:\n",
    "        s = \"\"\n",
    "        for i in range(len(num)):\n",
    "            s += type_list[i][num[i]]\n",
    "        result.append(s)\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"def predict(path_to_csv):\\n\\n    df = pd.read_csv(path_to_csv)\\n\\n    X, y = prep_data(df)\\n\\n    # loading the 4 models\\n    EorI_model = load(\\\"clf_is_Extrovert.joblib\\\")\\n    SorN_model = load(\\\"clf_is_Sensing.joblib\\\")\\n    TorF_model = load(\\\"clf_is_Thinking.joblib\\\")\\n    JorP_model = load(\\\"clf_is_Judging.joblib\\\")\\n\\n    # predicting\\n    EorI_pred = EorI_model.predict(X)\\n    print(\\\"y_true\\\", y[\\\"is_Extrovert\\\"].values)\\n    print(\\\"preds\\\", EorI_pred)\\n\\n    SorN_pred = SorN_model.predict(X)\\n    print(\\\"y_true\\\", y[\\\"is_Sensing\\\"].values)\\n    print(\\\"preds\\\", SorN_pred)\\n\\n    TorF_pred = TorF_model.predict(X)\\n    print(\\\"y_true\\\", y[\\\"is_Thinking\\\"].values)\\n    print(\\\"preds\\\", TorF_pred)\\n\\n    JorP_pred = JorP_model.predict(X)\\n    print(\\\"y_true\\\", y[\\\"is_Judging\\\"].values)\\n    print(\\\"preds\\\", JorP_pred)\\n\\n    # combining the predictions from the 4 models\\n    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\\n\\n    return result\";\n",
       "                var nbb_formatted_code = \"def predict(path_to_csv):\\n\\n    df = pd.read_csv(path_to_csv)\\n\\n    X, y = prep_data(df)\\n\\n    # loading the 4 models\\n    EorI_model = load(\\\"clf_is_Extrovert.joblib\\\")\\n    SorN_model = load(\\\"clf_is_Sensing.joblib\\\")\\n    TorF_model = load(\\\"clf_is_Thinking.joblib\\\")\\n    JorP_model = load(\\\"clf_is_Judging.joblib\\\")\\n\\n    # predicting\\n    EorI_pred = EorI_model.predict(X)\\n    print(\\\"y_true\\\", y[\\\"is_Extrovert\\\"].values)\\n    print(\\\"preds\\\", EorI_pred)\\n\\n    SorN_pred = SorN_model.predict(X)\\n    print(\\\"y_true\\\", y[\\\"is_Sensing\\\"].values)\\n    print(\\\"preds\\\", SorN_pred)\\n\\n    TorF_pred = TorF_model.predict(X)\\n    print(\\\"y_true\\\", y[\\\"is_Thinking\\\"].values)\\n    print(\\\"preds\\\", TorF_pred)\\n\\n    JorP_pred = JorP_model.predict(X)\\n    print(\\\"y_true\\\", y[\\\"is_Judging\\\"].values)\\n    print(\\\"preds\\\", JorP_pred)\\n\\n    # combining the predictions from the 4 models\\n    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\\n\\n    return result\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict(path_to_csv):\n",
    "\n",
    "    df = pd.read_csv(path_to_csv)\n",
    "\n",
    "    X, y = prep_data(df)\n",
    "\n",
    "    # loading the 4 models\n",
    "    EorI_model = load(\"clf_is_Extrovert.joblib\")\n",
    "    SorN_model = load(\"clf_is_Sensing.joblib\")\n",
    "    TorF_model = load(\"clf_is_Thinking.joblib\")\n",
    "    JorP_model = load(\"clf_is_Judging.joblib\")\n",
    "\n",
    "    # predicting\n",
    "    EorI_pred = EorI_model.predict(X)\n",
    "    print(\"y_true\", y[\"is_Extrovert\"].values)\n",
    "    print(\"preds\", EorI_pred)\n",
    "\n",
    "    SorN_pred = SorN_model.predict(X)\n",
    "    print(\"y_true\", y[\"is_Sensing\"].values)\n",
    "    print(\"preds\", SorN_pred)\n",
    "\n",
    "    TorF_pred = TorF_model.predict(X)\n",
    "    print(\"y_true\", y[\"is_Thinking\"].values)\n",
    "    print(\"preds\", TorF_pred)\n",
    "\n",
    "    JorP_pred = JorP_model.predict(X)\n",
    "    print(\"y_true\", y[\"is_Judging\"].values)\n",
    "    print(\"preds\", JorP_pred)\n",
    "\n",
    "    # combining the predictions from the 4 models\n",
    "    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Preprocessing Time: 0.9983274936676025 seconds\n",
      "y_true [1 1 0 0 1 0 1 0 0 0 0]\n",
      "preds [0 0 0 0 0 1 0 0 0 0 0]\n",
      "y_true [1 0 1 0 1 0 0 1 0 0 0]\n",
      "preds [0 0 1 0 1 0 0 1 0 0 0]\n",
      "y_true [1 0 0 1 0 0 0 0 0 1 1]\n",
      "preds [1 1 1 1 1 1 1 1 1 1 1]\n",
      "y_true [0 1 0 1 0 0 0 1 1 1 1]\n",
      "preds [1 1 0 1 0 0 0 0 0 0 0]\n",
      "\n",
      "\n",
      "['ESTP' 'ENFJ' 'ISFP' 'INTJ' 'ESFP' 'INFP' 'ENFP' 'ISFJ' 'INFJ' 'INTJ'\n",
      " 'INTJ']\n",
      "['INTJ', 'INTJ', 'ISTP', 'INTJ', 'ISTP', 'ENTP', 'INTP', 'ISTP', 'INTP', 'INTP', 'INTP']\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"if __name__ == \\\"__main__\\\":\\n\\n    predictions = predict(\\\"../data/test_data.csv\\\")\\n    y_truth = pd.read_csv(\\\"../data/test_data.csv\\\")[\\\"type\\\"].values\\n    \\n    print(\\\"\\\\n\\\")\\n    print(y_truth)\\n    print(predictions)\";\n",
       "                var nbb_formatted_code = \"if __name__ == \\\"__main__\\\":\\n\\n    predictions = predict(\\\"../data/test_data.csv\\\")\\n    y_truth = pd.read_csv(\\\"../data/test_data.csv\\\")[\\\"type\\\"].values\\n\\n    print(\\\"\\\\n\\\")\\n    print(y_truth)\\n    print(predictions)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    predictions = predict(\"../data/test_data.csv\")\n",
    "    y_truth = pd.read_csv(\"../data/test_data.csv\")[\"type\"].values\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(y_truth)\n",
    "    print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>ESTP</td>\n",
       "      <td>My thoughts and prayers are with the @USMC cre...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>ENFJ</td>\n",
       "      <td>Happy Hanukkah! Over these eight nights, we dr...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kanye West</td>\n",
       "      <td>ISFP</td>\n",
       "      <td>@jarrodspector @TheCherShow the dynamics of Ch...</td>\n",
       "      <td>ISTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arnold Schwarzenegger</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>Fantastic to spend some time with you teaming ...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>ESFP</td>\n",
       "      <td>All love over here Aaron. You got my support||...</td>\n",
       "      <td>ISTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kina Grannis</td>\n",
       "      <td>INFP</td>\n",
       "      <td>happiest of birthdays to this sweetest human i...</td>\n",
       "      <td>ENTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kristen Bell</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>NEW BOOK: “Congo Stories” shares the voices of...</td>\n",
       "      <td>INTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kim Kardashian</td>\n",
       "      <td>ISFJ</td>\n",
       "      <td>Very calm except for when she wants food lol S...</td>\n",
       "      <td>ISTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lady Gaga</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>#Enigma #GagaVegas https://t.co/lGl7cxSCAH|||#...</td>\n",
       "      <td>INTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elon Musk</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>Tracking shot of Falcon water landing https://...</td>\n",
       "      <td>INTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Brian Spiering</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>Yi Li @BaiduResearch doing reproducible resear...</td>\n",
       "      <td>INTP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name  type  \\\n",
       "0            Donald Trump  ESTP   \n",
       "1            Barack Obama  ENFJ   \n",
       "2              Kanye West  ISFP   \n",
       "3   Arnold Schwarzenegger  INTJ   \n",
       "4           Justin Bieber  ESFP   \n",
       "5            Kina Grannis  INFP   \n",
       "6            Kristen Bell  ENFP   \n",
       "7          Kim Kardashian  ISFJ   \n",
       "8               Lady Gaga  INFJ   \n",
       "9               Elon Musk  INTJ   \n",
       "10         Brian Spiering  INTJ   \n",
       "\n",
       "                                                posts result  \n",
       "0   My thoughts and prayers are with the @USMC cre...   INTJ  \n",
       "1   Happy Hanukkah! Over these eight nights, we dr...   INTJ  \n",
       "2   @jarrodspector @TheCherShow the dynamics of Ch...   ISTP  \n",
       "3   Fantastic to spend some time with you teaming ...   INTJ  \n",
       "4   All love over here Aaron. You got my support||...   ISTP  \n",
       "5   happiest of birthdays to this sweetest human i...   ENTP  \n",
       "6   NEW BOOK: “Congo Stories” shares the voices of...   INTP  \n",
       "7   Very calm except for when she wants food lol S...   ISTP  \n",
       "8   #Enigma #GagaVegas https://t.co/lGl7cxSCAH|||#...   INTP  \n",
       "9   Tracking shot of Falcon water landing https://...   INTP  \n",
       "10  Yi Li @BaiduResearch doing reproducible resear...   INTP  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"df[\\\"result\\\"] = predictions\\ndf\";\n",
       "                var nbb_formatted_code = \"df[\\\"result\\\"] = predictions\\ndf\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"result\"] = predictions\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project3",
   "language": "python",
   "name": "project3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
